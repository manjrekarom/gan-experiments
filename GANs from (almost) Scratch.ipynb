{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Discriminator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants and hyper-parameters\n",
    "# for Discriminator\n",
    "IMG_SIZE = 784\n",
    "DL1 = 500\n",
    "DLO = 1\n",
    "\n",
    "d_model = nn.Sequential(\n",
    "    nn.Linear(IMG_SIZE, DL1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(DL1, DLO),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "d_model = d_model.cuda(device=device)\n",
    "\n",
    "# define constants and hyper-parameters\n",
    "# for Generator\n",
    "NOISE_SIZE = 64\n",
    "GL1 = 500\n",
    "GL2 = 500\n",
    "GLO = 784\n",
    "\n",
    "g_model = nn.Sequential(\n",
    "    nn.Linear(NOISE_SIZE, GL1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(GL1, GL2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(GL2, GLO),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "g_model = g_model.cuda(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions and optimizers\n",
    "d_lr = 0.0002\n",
    "g_lr = 0.0002\n",
    "\n",
    "d_loss_fn = nn.BCELoss()\n",
    "d_opt = optim.SGD(d_model.parameters(), lr=d_lr, momentum=0.9)\n",
    "\n",
    "g_loss_fn = nn.BCELoss()\n",
    "g_opt = optim.Adam(g_model.parameters(), lr=g_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more hyper-parameters\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data iterator\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST(\n",
    "        './data', \n",
    "        train=True, \n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    ), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def denorm(x):\n",
    "    x = (x + 1)/2\n",
    "    return x.clamp(0, 1)\n",
    "    \n",
    "def sample_images():\n",
    "    n = 20\n",
    "    for i in range(n):\n",
    "        fake_images = g_model(torch.randn([n, NOISE_SIZE], device=device))\n",
    "        fake_images = denorm(fake_images)\n",
    "        fake_images = fake_images.reshape(n, 28, 28)\n",
    "        \n",
    "        plt.subplot(4, 5, i+1)\n",
    "        plt.imshow(denorm(fake_images[i]).detach().to('cpu').numpy())\n",
    "    \n",
    "    plt.pause(0.5)\n",
    "    plt.close()\n",
    "    \n",
    "def sample_z(batch_size, noise_size, device):\n",
    "    return torch.autograd.Variable(torch.distributions.Normal(0, 2).sample([batch_size, noise_size])).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 300\n",
    "d_steps = 1\n",
    "g_steps = 1\n",
    "\n",
    "print_iter = 200\n",
    "sample_iter = 500\n",
    "\n",
    "dlr, dlf, gl, dx, dgz = 0, 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_images, real_labels) in enumerate(train_loader):\n",
    "        # clear gradients\n",
    "        d_model.zero_grad()\n",
    "        g_model.zero_grad()\n",
    "\n",
    "        # train on real data\n",
    "        real_images = real_images.reshape(BATCH_SIZE, -1).to(device) # X\n",
    "\n",
    "        d_x = d_model(torch.autograd.Variable(real_images)) # D(X)\n",
    "        d_loss_real = d_loss_fn(d_x, torch.autograd.Variable(torch.ones([BATCH_SIZE, 1], device=device))) # log(D(X))\n",
    "        d_loss_real.backward()\n",
    "\n",
    "        # train on fake data\n",
    "        fake_images = g_model(sample_z(BATCH_SIZE, NOISE_SIZE, device)) # G(Z)\n",
    "\n",
    "        d_g_z = d_model(fake_images) # D(G(Z))\n",
    "        d_loss_fake = d_loss_fn(d_g_z, torch.autograd.Variable(torch.zeros([BATCH_SIZE, 1], device=device))) # log(1 - D(G(Z)))\n",
    "        d_loss_fake.backward()\n",
    "\n",
    "        d_opt.step()\n",
    "\n",
    "        dlr, dlf = extract(d_loss_real)[0], extract(d_loss_fake)[0]\n",
    "    \n",
    "        # clear gradients\n",
    "        d_model.zero_grad()\n",
    "        g_model.zero_grad()\n",
    "\n",
    "        # train on fake data\n",
    "        fake_images = g_model(sample_z(BATCH_SIZE, NOISE_SIZE, device)) # G(Z)\n",
    "\n",
    "        d_g_z = d_model(fake_images) # D(G(Z))\n",
    "        g_loss = g_loss_fn(d_g_z, torch.autograd.Variable(torch.ones([BATCH_SIZE, 1], device=device))) # log(1 - D(G(Z)))\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "\n",
    "        gl = extract(g_loss)[0]\n",
    "    \n",
    "        if i % print_iter == 0:\n",
    "            print(\"Epoch %s/%s: DReal: %s, DFake: %s, G: %s;    Dx: %s, Dg: %s\" % (epoch, i, dlr, dlf, gl, d_x.mean(), d_g_z.mean()))\n",
    "\n",
    "        if i % sample_iter == 0:\n",
    "            sample_images()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
